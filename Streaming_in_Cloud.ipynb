{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting boto3\n",
      "  Downloading boto3-1.17.6-py2.py3-none-any.whl (130 kB)\n",
      "\u001b[K     |████████████████████████████████| 130 kB 4.5 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting jmespath<1.0.0,>=0.7.1\n",
      "  Downloading jmespath-0.10.0-py2.py3-none-any.whl (24 kB)\n",
      "Collecting s3transfer<0.4.0,>=0.3.0\n",
      "  Downloading s3transfer-0.3.4-py2.py3-none-any.whl (69 kB)\n",
      "\u001b[K     |████████████████████████████████| 69 kB 3.9 MB/s eta 0:00:011\n",
      "\u001b[?25hCollecting botocore<1.21.0,>=1.20.6\n",
      "  Downloading botocore-1.20.6-py2.py3-none-any.whl (7.2 MB)\n",
      "\u001b[K     |████████████████████████████████| 7.2 MB 3.9 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: python-dateutil<3.0.0,>=2.1 in ./opt/anaconda3/lib/python3.8/site-packages (from botocore<1.21.0,>=1.20.6->boto3) (2.8.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.25.4 in ./opt/anaconda3/lib/python3.8/site-packages (from botocore<1.21.0,>=1.20.6->boto3) (1.25.11)\n",
      "Requirement already satisfied: six>=1.5 in ./opt/anaconda3/lib/python3.8/site-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.21.0,>=1.20.6->boto3) (1.15.0)\n",
      "Installing collected packages: jmespath, botocore, s3transfer, boto3\n",
      "Successfully installed boto3-1.17.6 botocore-1.20.6 jmespath-0.10.0 s3transfer-0.3.4\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install boto3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'AWS_KEY_ID' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-97f74c9719db>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mboto3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mfirehose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mboto3\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'firehose'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maws_access_key_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mAWS_KEY_ID\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maws_access_secret_key\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mAW_SECRET\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mregion_name\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;34m'us_east_1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'AWS_KEY_ID' is not defined"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "firehose=boto3.client('firehose',aws_access_key_id=AWS_KEY_ID, aws_access_secret_key= AW_SECRET, region_name= 'us_east_1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response=firehose.list_delivery_streams()\n",
    "print(response['DeliveryStreamNames']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Delete Old Stream Names\n",
    "response= firehose.list_delivery_streams()\n",
    "#Delete Them All\n",
    "for stream_name in response['DeliveryStreamNames']\n",
    "firehose.delete_delivery_stream(DeliveryStreamName=stream_name)\n",
    "#another example\n",
    "\n",
    "# Import boto3 and create boto3 Firehose client\n",
    "import boto3\n",
    "firehose = boto3.client('firehose', \n",
    "    aws_access_key_id=AWS_KEY_ID, aws_secret_access_key=AWS_SECRET, \n",
    "    region_name='us-east-1', endpoint_url=endpoints['FIREHOSE'])\n",
    "\n",
    "# Get list of delivery streams\n",
    "response = firehose.list_delivery_streams()\n",
    "\n",
    "# Iterate over the response contents and delete every stream\n",
    "for stream_name in response['DeliveryStreamNames']:\n",
    "    firehose.delete_delivery_stream(DeliveryStreamName=stream_name)\n",
    "    print(f\"Deleted stream: {stream_name}\")\n",
    "\n",
    "\n",
    "# Print list of delivery streams\n",
    "print(firehose.list_delivery_streams())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating Roles\n",
    "\n",
    "s3.create_bucket(Bucket='sd-vehicle-data')\n",
    "\n",
    "# List the buckets in S3\n",
    "for bucket_info in s3.list_buckets()['Buckets']:\n",
    "    \n",
    "    # Get the bucket_name\n",
    "    bucket_name = bucket_info['Name']\n",
    "    \n",
    "    # Generate bucket ARN.\n",
    "    arn = \"arn:aws:s3:::logs-bucket\".format(bucket_name)\n",
    "    \n",
    "    # Print the ARN\n",
    "    print(arn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: Invalid requirement: '_setup'\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "res=firehose.create_delivery_stream(DeliveryStreamName=\"gps-delivery-stream\",\n",
    "                                    DeliveryStreamType=\"DirectPut\" # we are writing stream directly,\n",
    "                                   ,S3DestinationConfiguration={\"RoleARN\": \"...\",\n",
    "                                                                \"BucketARN\":\"...\"})\n",
    "print(res['DeliveryStreamARN'])\n",
    "#back with Delivery Stream ARN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create_firehose.py: Create firehose stream. No need to edit.\n",
    "import _setup\n",
    "firehose, s3 = _setup.ex_vars\n",
    "\n",
    "# Create s3 bucket\n",
    "s3.create_bucket(Bucket='sd-vehicle-data')\n",
    "\n",
    "# Create Firehose delivery stream\n",
    "res = firehose.create_delivery_stream(\n",
    "    DeliveryStreamName=\"gps-delivery-stream\",\n",
    "    DeliveryStreamType=\"DirectPut\",\n",
    "    # Specify configuration of the destination S3 bucket\n",
    "    S3DestinationConfiguration = {\n",
    "        \"BucketARN\": \"arn:aws:s3:::sd-vehicle-data\",\n",
    "        \"RoleARN\": \"arn:aws:iam::0000000:role/firehoseDeliveryRole\"\n",
    "    })\n",
    "    \n",
    "# Print the stream ARN\n",
    "print(\"Created Firehose Stream ARN: {}\".format(res['DeliveryStreamARN']))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OBD2_sensors.py: Write to Firehose stream. EDIT HERE.\n",
    "import _setup, create_firehose\n",
    "firehose, s3, records = _setup.ex_vars\n",
    "for idx, row in records.iterrows(): \n",
    "\n",
    "    # Create a payload string that ends with a newline\n",
    "    payload = ' '.join(str(value) for value in row) \n",
    "    payload = payload + \"\\n\"\n",
    "    print(\"Sending payload: {}\".format(payload))\n",
    "\n",
    "    # Send the payload string to Firehose stream\n",
    "    res = firehose.put_record(\n",
    "        DeliveryStreamName = 'gps-delivery-stream',\n",
    "        Record = {'Data': payload})\n",
    "\n",
    "    # Print the written RecordId\n",
    "    print(\"Wrote to RecordId: {}\".format(res['RecordId']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List the objects that have been written to the S3 bucket\n",
    "objects = s3.list_objects(Bucket='sd-vehicle-data')['Contents']\n",
    "\n",
    "# Create list for collecting dataframes from read files.\n",
    "dfs = []\n",
    "\n",
    "# For every object, load it from S3.\n",
    "for obj in objects:\n",
    "    data_file = s3.get_object(Bucket='sd-vehicle-data', Key=obj['Key'])\n",
    "\n",
    "    # Load it into a dataframe, specifying a delimiter and column names\n",
    "    dfs.append(pd.read_csv(data_file['Body'], \n",
    "                           delimiter = \" \", \n",
    "                           names=[\"record_id\", \"timestamp\", \"vin\", \"lon\", \"lat\", \"speed\"]))\n",
    "\n",
    "# Concatenate the resulting dataframes.\n",
    "data = pd.concat(dfs)\n",
    "print(data.groupby(['vin'])['speed'].max())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
